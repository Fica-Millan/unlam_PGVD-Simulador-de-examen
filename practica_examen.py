import streamlit as st
import random
import time

# ------------------------------------------------------------
# Banco de preguntas (6 clases ‚Äì 120 preguntas)
# ------------------------------------------------------------

questions = {
    "Clase 01 ‚Äì Introducci√≥n a Big Data": [
        ("¬øCu√°l es una raz√≥n clave del surgimiento del Big Data?",
        ["Eliminaci√≥n de redes sociales","Decrecimiento del e-commerce","Reducci√≥n de costos en almacenamiento y c√≥mputo","Falta de datos disponibles"], 2),
        
        ("¬øCu√°l de las siguientes NO es una V del Big Data?",
        ["Velocidad","Validaci√≥n","Variedad","Volumen"], 1),
        
        ("El tipo de dato que representa el 80% del total es:",
        ["Datos estructurados","Datos semiestructurados","Datos no estructurados","Datos relacionales"], 2),
        
        ("¬øQu√© tipo de problema requiere baja latencia?",
        ["Batch","Offline","Online / tiempo real","Archivo hist√≥rico"], 2),
        
        ("¬øQu√© tecnolog√≠a permite conectar varias computadoras para escalar tareas?",
        ["HDFS","Computaci√≥n distribuida","Blockchain","FTP"], 1),
        
        ("¬øQu√© tipo de base de datos incluye MongoDB y HBase?",
        ["Relacionales","Columnar-relacional","NoSQL","SQL tradicionales"], 2),
        
        ("¬øCu√°l es un caso de uso t√≠pico del Big Data?",
        ["Ordenar carpetas en un disco local","An√°lisis de redes sociales","Creaci√≥n de documentos PDF","Instalaci√≥n de sistemas operativos"], 1),
        
        ("¬øQu√© describe mejor el valor en Big Data?",
        ["Cantidad de datos procesados","Utilidad obtenida de los datos","N√∫mero de servidores","Tama√±o de la red"], 1),
        
        ("La computaci√≥n en la nube aporta principalmente:",
        ["Mayor complejidad","Escalabilidad el√°stica","Menor latencia garantizada","Eliminaci√≥n de bases de datos"], 1),
        
        ("Los datos semiestructurados‚Ä¶",
        ["Tienen esquema fijo","No poseen ning√∫n tipo de estructura","Poseen estructura parcial","Solo existen en RDBMS"], 2),
        
        ("'Escriba una vez, lea muchas veces' se refiere a:",
        ["RDBMS","Redes TCP","Big Data","HDFS"], 3),
        
        ("¬øCu√°l es un ejemplo de dato no estructurado?",
        ["Fecha","N√∫mero entero","Imagen","ID de usuario"], 2),
        
        ("¬øQu√© V se relaciona con sensores en tiempo real?",
        ["Volumen","Valor","Variedad","Velocidad"], 3),
        
        ("El contenido multimedia pertenece a:",
        ["Datos estructurados","Datos no estructurados","Datos tabulares","Datos binarios √∫nicamente"], 1),
        
        ("Batch se usa principalmente para:",
        ["Detecci√≥n de fraude en milisegundos","Procesar datos hist√≥ricos masivos","Juegos en l√≠nea","Procesos embebidos"], 1),
        
        ("La pila tecnol√≥gica de Big Data comienza por:",
        ["Capa de seguridad","Infraestructura f√≠sica redundante","Capa de an√°lisis","Interfaces de usuario"], 1),
        
        ("En las capas de la pila, la capa 3 incluye:",
        ["SQL","Integraciones m√≥viles","MapReduce y Hadoop","GPUs"], 2),
        
        ("Las bases de datos operativas se encuentran en:",
        ["Capa 0","Capa 1","Capa 2","Capa 4"], 2),
        
        ("¬øQu√© requiere la integraci√≥n para que Big Data sea posible?",
        ["Interfaces entre capas","Instalaci√≥n de drivers","Computadoras de alta gama","Conexi√≥n 5G"], 0),
        
        ("Big Data NO es‚Ä¶",
        ["Una sola tecnolog√≠a","Una combinaci√≥n de tecnolog√≠as","Utilizado para extraer valor","El resultado de la convergencia tecnol√≥gica"], 0),
    ],


    "Clase 02 ‚Äì Hadoop, HDFS, MapReduce": [
        ("¬øQui√©n desarroll√≥ originalmente Hadoop?",
        ["IBM", "Microsoft", "Yahoo!", "Google"], 2),

        ("Hadoop est√° dise√±ado para:",
        ["Procesamiento online", "Procesamiento batch", "Lectura aleatoria r√°pida", "Bases SQL tradicionales"], 1),

        ("¬øQu√© componente gestiona recursos en Hadoop?",
        ["Hive", "YARN", "HBase", "Sqoop"], 1),

        ("¬øQu√© tama√±o de bloque tiene HDFS por defecto?",
        ["1 MB", "16 MB", "64 MB", "1 GB"], 2),

        ("En HDFS, ¬ød√≥nde se almacenan los metadatos?",
        ["DataNode", "NameNode", "Sqoop", "Hive"], 1),

        ("¬øQu√© componente permite acceso aleatorio en tiempo real?",
        ["HDFS", "MapReduce", "HBase", "Pig"], 2),

        ("MapReduce utiliza como entrada:",
        ["Datos JSON", "Pares clave-valor", "Im√°genes", "SQL"], 1),

        ("¬øQu√© fase de MapReduce ordena la salida intermedia?",
        ["Map", "Reduce", "Sort", "Combine"], 2),

        ("¬øQu√© fase mueve los datos intermedios a los reducers?",
        ["Map", "Shuffle", "Sort", "Almacenamiento"], 1),

        ("¬øQu√© proyecto permite scripts simples para Hadoop?",
        ["Hive", "Pig", "Zookeeper", "Flume"], 1),

        ("¬øCu√°l es una ventaja de MapReduce?",
        ["No es escalable", "No tolera fallos", "Oculta la complejidad del cluster", "Permite lectura aleatoria"], 2),

        ("En MapReduce, la funci√≥n map es:",
        ["Mutable", "No conmutativa", "Inmutable y conmutativa", "No paralelizable"], 2),

        ("La replicaci√≥n t√≠pica en HDFS es:",
        ["1", "2", "3", "10"], 2),

        ("¬øQu√© componente coordina sincronizaci√≥n en sistemas distribuidos?",
        ["HDFS", "Pig", "Zookeeper", "Sqoop"], 2),

        ("Hadoop funciona mejor sobre:",
        ["Hardware caro", "Hardware commodity", "Procesadores GPU", "Mainframes"], 1),

        ("Hive utiliza un lenguaje similar a SQL llamado:",
        ["MySQL", "SparkSQL", "HiveQL", "PigLatin"], 2),

        ("Sqoop sirve para:",
        ["Streaming", "Importar/exportar datos entre SQL y Hadoop", "Crear clusters", "Procesar logs"], 1),

        ("La filosof√≠a 'mover el c√≥digo al dato' pertenece a:",
        ["SQL", "HDFS", "Hadoop", "Kafka"], 2),

        ("MapReduce fue creado por:",
        ["Google", "Facebook", "Microsoft", "Netflix"], 0),

        ("¬øQu√© componente env√≠a heartbeats al NameNode?",
        ["Hive", "DataNode", "Zookeeper", "Client"], 1)
    ],


    "Clase 03 ‚Äì Spark, RDD, DataFrames, SQL": [
        ("Spark fue creado en:",
        ["Yahoo!", "MIT", "UC Berkeley", "IBM"], 2),

        ("Spark es aproximadamente _____ veces m√°s r√°pido que Hadoop MapReduce.",
        ["2", "10", "50", "100"], 3),

        ("La pieza central de conexi√≥n al cluster es:",
        ["SparkDriver", "SparkShell", "SparkContext", "SparkCore"], 2),

        ("¬øC√≥mo se llama el m√≥dulo para datos estructurados?",
        ["Spark Stream", "Spark SQL", "GraphX", "MLlib"], 1),

        ("Un RDD es‚Ä¶",
        ["Mutable", "Inmutable", "Un archivo f√≠sico", "Una base SQL"], 1),

        ("Los DataFrames se definen como:",
        ["RDDs con esquema", "RDDs sin particiones", "Tablas SQL normales", "Conjuntos binarios crudos"], 0),

        ("Spark Streaming trabaja mediante:",
        ["Streaming puro", "Micro-batches", "Hilos paralelos", "SQL iterativo"], 1),

        ("¬øQu√© m√≥dulo se usa para machine learning?",
        ["GraphX", "Spark SQL", "MLlib", "Flume"], 2),

        ("GraphX permite trabajar con:",
        ["Archivos CSV", "Grafos", "Im√°genes", "Streams financieros"], 1),

        ("Los shells interactivos de Spark incluyen:",
        ["Python y C++", "Python y Scala", "R y Go", "Java y C"], 1),

        ("Para scripts se utiliza:",
        ["spark-run", "spark-init", "spark-submit", "spark-exec"], 2),

        ("Los RDDs son tolerantes a fallos mediante:",
        ["Replicaci√≥n", "DAG de operaciones", "Hashing", "Integridad binaria"], 1),

        ("¬øQu√© lenguaje se usa en HiveQL?",
        ["SQL extendido", "Python", "Scala nativo", "Bash"], 0),

        ("En Spark Streaming, un DStream es:",
        ["Una tabla", "Un RDD distribuido √∫nico", "Una secuencia de RDDs", "Un archivo en HDFS"], 2),

        ("Spark Core se encarga de:",
        ["Machine learning", "Grafos", "Scheduling y funciones b√°sicas", "Manejo de im√°genes"], 2),

        ("¬øQu√© formato NO se menciona como compatible con Spark SQL?",
        ["JSON", "JDBC", "ODBC", "XML"], 3),

        ("Spark integra por defecto con:",
        ["HDFS", "Docker", "Prometheus", "Airflow"], 0),

        ("Un DataFrame est√° compuesto por:",
        ["Archivos", "Tuplas con nombre y tipo", "Bytes sin procesar", "Listas sin esquema"], 1),

        ("¬øQu√© lenguaje incluye spark-shell?",
        ["Python", "Scala", "Java", "SQL"], 1),

        ("El driver en Spark:",
        ["Ejecuta operaciones en los nodos",
        "Maneja la l√≥gica y env√≠a tareas",
        "Es un DataFrame",
        "Es el sistema de archivos"], 
        1),
    ],


    "Clase 04 ‚Äì Spark Streaming, MLlib, GraphX": [
        ("Spark Streaming funciona mediante:", 
        ["Streaming 100% continuo", "Micro-batches", "Lectura por lotes diarios", "SQL en tiempo real"], 
        1),

        ("Un DStream est√° compuesto por:", 
        ["Una sola RDD", "Un archivo JSON", "Una secuencia de RDDs", "Un DataFrame temporal"], 
        2),

        ("Spark Streaming NO es 100% streaming debido a:", 
        ["Su arquitectura", "La latencia de Kafka", "El uso de micro-batches", "Limitaciones de HDFS"], 
        2),

        ("Una ventana que conserva los √∫ltimos n datos se llama:", 
        ["Landmark", "Sliding", "Fading", "Growing"], 
        1),

        ("¬øCu√°l es una fuente t√≠pica de streaming?", 
        ["CSV", "Amazon S3", "Twitter", "Parquet"], 
        2),

        ("Un algoritmo de streaming debe priorizar:", 
        ["Precisi√≥n sin l√≠mite", "Velocidad, memoria y eficacia", "Tama√±o del cluster", "N√∫mero de particiones"], 
        1),

        ("Un DStream representa:", 
        ["Un RDD continuo", "Datos estructurados", "Flujos discretizados", "Tablas SQL"], 
        2),

        ("¬øQu√© fuente NO es mencionada para Spark Streaming?", 
        ["Kafka", "Twitter", "Flume", "Google Drive"], 
        3),

        ("La ventana Sliding Window:", 
        ["Mantiene todos los datos hist√≥ricos", "Se mueve en intervalos fijos", "Aumenta infinitamente", "Solo sirve para MLlib"], 
        1),

        ("En MLlib, los algoritmos son:", 
        ["No distribuidos", "Dise√±ados para RAM local", "Iterativos y distribuidos", "Basados en GPUs"], 
        2),

        ("¬øQu√© algoritmo NO pertenece a MLlib?", 
        ["K-Means", "Random Forest", "√Årboles impulsados por gradiente", "Algoritmos de ordenamiento"], 
        3),

        ("MLlib puede realizar:", 
        ["Clustering", "Procesamiento de im√°genes", "Renderizado 3D", "Compilaci√≥n de c√≥digo"], 
        0),

        ("¬øCu√°l NO es un algoritmo de clustering?", 
        ["Gaussian Mixtures", "LDA", "K-means", "Random Forest"], 
        3),

        ("GraphX se usa para trabajar con:", 
        ["HDFS", "Grafos", "Bases relacionales", "Archivos XML"], 
        1),

        ("PageRank sirve para:", 
        ["Clasificaci√≥n", "Detecci√≥n de duplicados", "Medir importancia de nodos", "Hacer compresi√≥n de grafos"], 
        2),

        ("Strongly Connected Components identifica:", 
        ["Una √∫nica comunidad", "Componentes conectados por fuerza", "Componentes fuertemente conectados", "Solo nodos sin relaciones"], 
        2),

        ("Triangle Count se usa para:", 
        ["Contar nodos", "Contar tri√°ngulos", "Medir latencia", "Detectar outliers"], 
        1),

        ("GraphX es parte de:", 
        ["Hadoop", "Kubernetes", "Spark", "Hive"], 
        2),

        ("Un uso t√≠pico de GraphX es:", 
        ["Limpieza de datos tabulares", "Detecci√≥n de fraudes", "Exportar CSV", "Entrenar modelos NLP"], 
        1),

        ("Un modelo de streaming puede:", 
        ["Entrenarse din√°micamente", "No actualizarse nunca", "Trabajar solo offline", "Depender solo de HDFS"], 
        0)
    ],


    "Clase 05 ‚Äì Containers y Docker": [
        ("¬øCu√°l es el problema que motiv√≥ el uso de containers?",
        ["Falta de RAM", "‚ÄúEn mi m√°quina funciona‚Ä¶‚Äù", "Incompatibilidad de redes", "Escasez de CPUs"],
        1),

        ("Un contenedor incluye:",
        ["Un OS completo", "Todos los paquetes del host", "Solo lo necesario para la aplicaci√≥n", "Un kernel propio"],
        2),

        ("¬øQu√© caracter√≠stica NO es propia de un contenedor?",
        ["Ligero", "Portable", "Aislado", "Arranque lento"],
        3),

        ("Las im√°genes son:",
        ["Contenedores en ejecuci√≥n", "Archivos sin estado", "Binarios del procesador", "Bases de datos"],
        1),

        ("Los contenedores se crean a partir de:",
        ["Repositorios", "Im√°genes", "Kernels", "Procesos del SO"],
        1),

        ("¬øQu√© diferencia a una VM de un contenedor?",
        ["La VM incluye un OS completo", "La VM arranca m√°s r√°pido", "Los contenedores pesan GB", "Los contenedores no se a√≠slan"],
        0),

        ("Docker Engine consiste en:",
        ["Cliente, servidor y API", "Driver y shell", "Kernel y red", "Daemon y GPU"],
        0),

        ("El daemon de Docker se llama:",
        ["docker-engine", "docker-daemon", "dockerd", "docker-run"],
        2),

        ("El cliente Docker se comunica con el daemon mediante:",
        ["API REST", "SSH", "FTP", "SQL"],
        0),

        ("¬øQu√© mide el tama√±o de un contenedor t√≠picamente?",
        ["KB", "MB", "GB", "TB"],
        1),

        ("Un contenedor es:",
        ["Un proceso", "Un archivo", "Una VM", "Un hypervisor"],
        0),

        ("Las im√°genes pueden:",
        ["Tener estado", "Iniciar procesos", "Moverse entre m√°quinas", "Modificar el kernel"],
        2),

        ("Las VMs se caracterizan por:",
        ["Arranques muy r√°pidos", "Aislamiento ligero", "Uso de un OS completo", "Reutilizar kernel del host"],
        2),

        ("Un caso t√≠pico de uso de containers es:",
        ["Correr sistemas operativos completos", "Empaquetar microservicios", "Administrar hardware", "Montar redes f√≠sicas"],
        1),

        ("Los containers comparten:",
        ["El kernel del host", "El filesystem del host", "Usuarios del host", "Drivers del host"],
        0),

        ("¬øQu√© afirma la met√°fora del ‚Äúremolque‚Äù?",
        ["Los contenedores incluyen motor", "Las VMs son m√°s ligeras", "Los contenedores usan el kernel del host", "Las VMs comparten kernel"],
        2),

        ("Una imagen incluye:",
        ["El kernel", "Archivos necesarios para ejecutar una app", "El scheduler del sistema", "Un servidor web siempre"],
        1),

        ("La portabilidad se debe a:",
        ["Im√°genes siempre id√©nticas", "Uso de redes privadas", "Uso de drivers comunes", "Falta de dependencias"],
        0),

        ("¬øQu√© comando/lista corresponde a una VM?",
        ["GB de tama√±o", "MB de tama√±o", "Kernel compartido", "Inicio casi instant√°neo"],
        0),

        ("Los contenedores permiten resolver:",
        ["Altos costos", "Conflictos de dependencias", "Falta de datos", "Problemas de visualizaci√≥n"],
        1),
    ],


    "Clase 06 ‚Äì Kafka": [
        ("Kafka es principalmente un:", 
        ["Sistema de archivos", "Sistema de mensajer√≠a distribuida", "Motor SQL", "Renderizador"], 
        1),

        ("Los servidores que almacenan mensajes se llaman:", 
        ["DataNodes", "Brokers", "Producers", "Agents"], 
        1),

        ("Kafka utiliza para coordinaci√≥n:", 
        ["HDFS", "Zookeeper", "MLlib", "JDBC"], 
        1),

        ("Un producer se encarga de:", 
        ["Leer mensajes", "Crear y publicar mensajes", "Controlar particiones", "Ordenar offsets"], 
        1),

        ("Un consumer se encarga de:", 
        ["Publicar mensajes", "Eliminar particiones", "Leer mensajes", "Replicar brokers"], 
        2),

        ("Los mensajes se agrupan en:", 
        ["Streams", "T√≥picos", "Jobs", "Hilos"], 
        1),

        ("El orden de Kafka est√° garantizado:", 
        ["En todo el cluster", "Entre particiones √∫nicamente", "Solo dentro de una partici√≥n", "No est√° garantizado"], 
        2),

        ("La pol√≠tica de retenci√≥n define:", 
        ["El orden de los mensajes", "Cu√°ndo se borran mensajes", "Qu√© consumer los recibe", "La prioridad en redes"], 
        1),

        ("Una partici√≥n contiene:", 
        ["Mensajes en paralelo", "Mensajes con timestamp", "Offset y mensajes", "Solo claves"], 
        2),

        ("El offset representa:", 
        ["La hora del servidor", "La posici√≥n del mensaje", "El tama√±o del t√≥pico", "La clave del mensaje"], 
        1),

        ("La ventaja de la retenci√≥n es:", 
        ["M√°s capacidad de borrado", "Lectura repetida de mensajes", "Seguridad", "Evita duplicados"], 
        1),

        ("¬øQu√© algoritmo se usa para elegir partici√≥n por defecto?", 
        ["Hash por clave", "Round robin", "Prioridad", "Tiempo de llegada"], 
        1),

        ("Kafka garantiza tolerancia a fallos mediante:", 
        ["Redundancia de offsets", "Replicaci√≥n de particiones", "Zookeeper distribuido", "Duplicaci√≥n de producers"], 
        1),

        ("Kafka NO elimina mensajes autom√°ticamente porque:", 
        ["Es un sistema batch", "Requiere pol√≠tica de retenci√≥n", "No soporta borrado", "Usa solo RAM"], 
        1),

        ("Streams API permite:", 
        ["Crear t√≥picos", "Crear pipelines de procesamiento", "Configurar offsets", "Administrar Zookeeper"], 
        1),

        ("Connect API permite:", 
        ["Enviar datos a sistemas externos", "Manejar particiones", "Ajustar retenci√≥n", "Replicar brokers"], 
        0),

        ("Kafka es ideal para:", 
        ["Procesamiento offline", "Flujo continuo de datos", "SQL intensivo", "Sistemas monol√≠ticos"], 
        1),

        ("Kafka mantiene orden dentro de una partici√≥n porque:", 
        ["Usa √≠ndices invertidos", "Los mensajes se agregan secuencialmente", "Se ordenan por timestamp", "Usa √°rboles B+"], 
        1),

        ("¬øQu√© NO pertenece al ecosistema de Kafka?", 
        ["Brokers", "Producers", "Consumers", "DataNodes"], 
        3),

        ("El timestamp del mensaje incluye:", 
        ["Offset", "Hora en 64 bits", "Partici√≥n", "Hash de clave"], 
        1),
    ]

}

# ---------------------------
# Agregar la opci√≥n integradora
# ---------------------------
clases_existentes = list(questions.keys())
opcion_integradora = "üîÄ Integrador (todas las clases)"
clases_mostradas = clases_existentes + [opcion_integradora]

# ---------------------------
# Cron√≥metro
# ---------------------------
if "start_time" not in st.session_state:
    st.session_state.start_time = None

def iniciar_cronometro():
    st.session_state.start_time = time.time()

def obtener_tiempo_transcurrido():
    if st.session_state.start_time is None:
        return "00:00"
    t = int(time.time() - st.session_state.start_time)
    mins = t // 60
    secs = t % 60
    return f"{mins:02d}:{secs:02d}"

# ------------------------------------------------------------
# Configuraci√≥n de la p√°gina
# ------------------------------------------------------------
st.title("Simulador de Examen")
st.markdown("Selecciona una clase para comenzar el examen.")

# ------------------------------------------------------------
# Selecci√≥n de clase
# ------------------------------------------------------------
clase = st.selectbox("üìò Selecciona la clase:", clases_mostradas)

# ------------------------------------------------------------
# Inicializaci√≥n de sesi√≥n
# ------------------------------------------------------------
if "selected_questions" not in st.session_state:
    st.session_state.selected_questions = []
if "answers" not in st.session_state:
    st.session_state.answers = {}
if "submitted" not in st.session_state:
    st.session_state.submitted = False

# ------------------------------------------------------------
# Generar examen
# ------------------------------------------------------------
if st.button("üéØ Generar examen de la clase"):
    st.session_state.submitted = False
    st.session_state.answers = {}

    # activar cronometro
    iniciar_cronometro()

    # Cargar preguntas seg√∫n la opci√≥n
    if clase == opcion_integradora:
        todas = []
        for c in clases_existentes:
            todas.extend(questions[c])
        num_questions = min(20, len(todas))  # integrador con 20 preguntas
        st.session_state.selected_questions = random.sample(todas, k=num_questions)
    else:
        num_questions = min(10, len(questions[clase]))
        st.session_state.selected_questions = random.sample(questions[clase], k=num_questions)

    st.success("Preguntas generadas. ¬°Pod√©s comenzar!")

# ------------------------------------------------------------
# Mostrar preguntas + Cron√≥metro
# ------------------------------------------------------------
if st.session_state.selected_questions and not st.session_state.submitted:

    # ‚è±Ô∏è CRON√ìMETRO (actualiza con cada interacci√≥n)
    st.markdown(f"### ‚è±Ô∏è Tiempo transcurrido: **{obtener_tiempo_transcurrido()}**")

    st.header(f"üìö Examen ‚Äì {clase}")
    st.markdown("Seleccion√° una respuesta para cada pregunta.")

    for idx, (q, options, ans_index) in enumerate(st.session_state.selected_questions):
        st.session_state.answers[idx] = st.radio(
            f"{idx+1}. {q}",
            options,
            key=f"q_{idx}"
        )

    if st.button("üìå Enviar examen"):
        st.session_state.submitted = True

# ------------------------------------------------------------
# Correcci√≥n
# ------------------------------------------------------------
if st.session_state.submitted:
    st.header("üìä Resultados del examen")

    tiempo_final = obtener_tiempo_transcurrido()
    st.markdown(f"‚è±Ô∏è **Tiempo total:** `{tiempo_final}`")

    score = 0
    for idx, (q, options, ans_index) in enumerate(st.session_state.selected_questions):
        user_answer = st.session_state.answers.get(idx, None)
        correct = options[ans_index]

        if user_answer == correct:
            score += 1
            st.success(f"‚úîÔ∏è {idx+1}. Correcta ‚Äì {q}")
        else:
            st.error(f"‚ùå {idx+1}. Incorrecta ‚Äì {q}\n**Respuesta correcta:** {correct}")

    st.markdown(f"### üü¶ Puntaje final: **{score} / {len(st.session_state.selected_questions)}**")

    if st.button("üîÑ Reiniciar"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()


# ------------------------------------------------------------
# FOOTER 
# ------------------------------------------------------------
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; font-size: 15px; color: gray;'>
        <p><b>Especializaci√≥n en Ciencia de Datos ‚Äì UNLaM</b><br>
        Esto lo hice para estudiar para la materia de <b>Procesamiento de Grandes Vol√∫menes de Datos</b>.</p>
        <p>Desarrollado por <b>Yesica Fica Mill√°n</b> ‚Äì <a href="https://www.linkedin.com/in/yesica-fica-millan" target="_blank">LinkedIn</a></p>
        <p style='font-size:13px;'>¬© 2025 ‚Äì Proyecto de estudio personal</p>
    </div>
    """,
    unsafe_allow_html=True
)
